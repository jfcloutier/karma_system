# About aboutness

I've been thinking about aboutness. Aboutness is at the very core of what this project is exploring.

I want (immodestly, perhaps foolishly) to program a simple LEGO robot to have felt experiences it can learn from so that it gets better at exploring and exploiting a dynamic environment for its own survival.

For this to happen, the robot needs, among other things, to make sense of its sensations (intero and extero) and to discover its affordances, all of this on its own. The robot must, through self-directed engagement with its world, grow and curate its own sense of aboutness.

I am writing a program to animate a robot and yet I must avoid imposing through this program my own sense of what the robot's world is about. The robot must -this is the whole point of this experiment- grow its own sense of the world, its own aboutness. Is this even possible? It certainly raises fundamental questions of epistemology, determinacy and agency (dare I say free will?)

The robot is pre-programmed, yes, but it is programmed to extend its own programming. It grows a (very basic) mentality, a hierarchy of cognitive processes, each with its own umwelt that transitively maps to the physical world. The cognitive processes discover causal theories to explain changes to their umwelts. To trigger this discovery, the robot decides, at first randomly and then purposely, how to act in its world so as to cause sensations it can learn from, all the while avoiding damage and, eventually and crucially, seeking out sources of "energy" to stave off "starvation".

Is the robot's self-programming fully determined by my own pre-programming of it? It would seem so at first glance but can't the robot's interactions with a messy real-world open up some room for self-determination? Will elements of randomness intentionally introduced by the pre-program, plus the robot's unpredictable historicity, allow for inherent purpose and agency to sneak in? I don't think there are obvious yes or no answers or else the question of our own free will would already be settled.

The robot's program, both the pre-coded and self-coded parts, are made up of symbols that are operated on via automated inferencing. Symbols, in and of themselves, are devoid of aboutness. When, for example, we say that A => B (if A is true then B is true) is synonymous with ~A V B (A is false or B is true), we say nothing about what A and B refer to. So how does aboutness enter into the robot's symbolic program? How are the meanings of its symbols grounded?

A philosophical stance I agree with is that the meanings of symbols, their aboutness, can only be grounded in the experiences of a mortal self. In GOFAI expert systems (Good Old-Fashioned Artificial Intelligence), the meanings of symbols reside entirely in the heads of programmers who got them from domain experts. The inferences an expert system makes are meaningless to the expert system in the same way a calculator does not understand what it calculates.

The (immodest and likely foolish) goal of this project is to have the robot create its own symbols that it then imbues with meaning grounded in its experiences as a mortal agent. Put another way, the challenge is to have a robot instantiate its own experiential aboutness so that we can reasonably ask the question "What is it like to be a sentient LEGO robot" and find this impossible to answer.
